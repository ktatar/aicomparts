<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Raw Music From Free Movements | AI in Computational Arts, Music, and Games </title> <meta name="author" content="Kıvanç Tatar"> <meta name="description" content="Human Body Pose Sequences into Audio Waveforms."> <meta name="keywords" content="Machine Learning, Artificial Intelligence, Computational Arts, Music, Video Games, Computational Creativity, Musical AI"> <link rel="stylesheet" href="/aicomparts/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/aicomparts/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/aicomparts/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/aicomparts/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ktatar.github.io//aicomparts/projects/ramfem/"> <script src="/aicomparts/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/aicomparts/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/aicomparts/assets/js/distillpub/template.v2.js"></script> <script src="/aicomparts/assets/js/distillpub/transforms.v2.js"></script> <script src="/aicomparts/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Raw Music From Free Movements",
            "description": "Human Body Pose Sequences into Audio Waveforms.",
            "published": "October 20, 2020",
            "authors": [
              
              {
                "author": "Kıvanç Tatar",
                "authorURL": "https://www.kivanctatar.com",
                "affiliations": [
                  {
                    "name": "Chalmers University of Technology",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Daniel Bisig",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Zurich University of the Arts",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/aicomparts//"> AI in Computational Arts, Music, and Games </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/aicomparts/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/aicomparts/about/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/aicomparts/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/aicomparts/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/aicomparts/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/aicomparts/people/">People </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Raw Music From Free Movements</h1> <p>Human Body Pose Sequences into Audio Waveforms.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#description">Description</a></li> <li class="toc-entry toc-h2"><a href="#architecture">Architecture</a></li> <li class="toc-entry toc-h2"><a href="#examples">Examples</a></li> <li class="toc-entry toc-h2"> <a href="#resources">Resources</a> <ul> <li class="toc-entry toc-h3"><a href="#code">Code</a></li> <li class="toc-entry toc-h3"><a href="#supplementary-material">Supplementary Material</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#acknowledgements">Acknowledgements</a></li> </ul> </nav> </d-contents> <h2 id="description">Description</h2> <p>Raw Music from Free Movements is a deep learning architecture that translates pose sequences into audio waveforms. The architecture combines a sequence-to-sequence model generating audio encodings and an adversarial autoencoder that generates raw audio from audio encodings. Experiments have been conducted with two datasets: a dancer improvising freely to a given music, and music created through simple movement sonification. The paper presents preliminary results. These will hopefully lead closer towards a model which can learn from the creative decisions a dancer makes when translating music into movement and then follow these decisions reversely for the purpose of generating music from movement.</p> <h2 id="architecture">Architecture</h2> <div class="fake-img l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/aicomparts/assets/img/ramfem-arch-480.webp 480w,/aicomparts/assets/img/ramfem-arch-800.webp 800w,/aicomparts/assets/img/ramfem-arch-1400.webp 1400w,/aicomparts/assets/img/ramfem-arch-1800.webp 1800w," sizes="95vw" type="image/webp"></source> <img src="/aicomparts/assets/img/ramfem-arch.png" width="100%" height="auto" title="The Deep Learning architecture in Raw Music From Free Movements" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> The Deep Learning architecture in Raw Music From Free Movements. </div> <h2 id="examples">Examples</h2> <p>Two different datasets were employed for training, named improvisation dataset and sonification dataset. The improvisation dataset consists of pose sequences and audio that have been recorded while a dancer was freely improvising to a given music. The dancer is an expert with a specialisation in contemporary dance and improvisation. The music consists of short excerpts of royalty free music including experimental electronic music, free jazz, and contemporary classic. The pose sequences have been acquired using the markerless motion capture system (The Captury ) in the iLab at MotionBank, University for Applied Research, Mainz. The recording is 10 minutes in length which corresponds to a sequence of 30000 poses. Each pose consists of 29 joints whose relative orientations are represented by quaternions.</p> <p>The sonification dataset contains the same pose sequences as the improvisation dataset. The audio of this dataset was created afterwards, through sonification, employing a very simple sound synthesis consisting of two sine oscillators controlled by the dancer’s hands. The frequency and amplitude of each oscillator are proportional to the height and velocity of the corresponding hand, respectively. The authors created this dataset to verify the performance of RAMFEM.</p> <h2 id="resources">Resources</h2> <h3 id="code">Code</h3> <p><i class="fa-brands fa-github"></i> <a>https://bitbucket.org/dbisig/rawmusicfromfreemovements</a></p> <h3 id="supplementary-material">Supplementary Material</h3> <p><i class="fa-solid fa-plus"></i> <a>https://zenodo.org/record/4656086</a></p> <h2 id="acknowledgements">Acknowledgements</h2> <p>This work has been supported by the Swiss National Science Foundation, the Natural Sciences and Engineering Research Council of Canada, and Social Sciences and Humanities Research Council of Canada.</p> <p>Ce travail est supporté par le Fonds national Suisse de la recherche scientifique, le Conseil national des sciences et de l’ingénieurie du Canada, et le Conseil national des sciences humaines et sociales du Canada.</p> <div class="fake-img l-page-outset"> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/aicomparts/assets/img/logos/marie-curie-480.webp 480w,/aicomparts/assets/img/logos/marie-curie-800.webp 800w,/aicomparts/assets/img/logos/marie-curie-1400.webp 1400w,/aicomparts/assets/img/logos/marie-curie-1800.webp 1800w," sizes="95vw" type="image/webp"></source> <img src="/aicomparts/assets/img/logos/marie-curie.png" width="100%" height="auto" title="Marie Curie logo" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/aicomparts/assets/img/logos/icst-480.webp 480w,/aicomparts/assets/img/logos/icst-800.webp 800w,/aicomparts/assets/img/logos/icst-1400.webp 1400w,/aicomparts/assets/img/logos/icst-1800.webp 1800w," sizes="95vw" type="image/webp"></source> <img src="/aicomparts/assets/img/logos/icst.png" width="100%" height="auto" title="Institute for Computer Music and Sound Technologies at Zurich University of the Arts, Switzerland" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/aicomparts/assets/img/logos/zhdk-480.webp 480w,/aicomparts/assets/img/logos/zhdk-800.webp 800w,/aicomparts/assets/img/logos/zhdk-1400.webp 1400w,/aicomparts/assets/img/logos/zhdk-1800.webp 1800w," sizes="95vw" type="image/webp"></source> <img src="/aicomparts/assets/img/logos/zhdk.png" width="100%" height="auto" title="Zurich University of the Arts logo" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/aicomparts/assets/img/logos/icst-480.webp 480w,/aicomparts/assets/img/logos/icst-800.webp 800w,/aicomparts/assets/img/logos/icst-1400.webp 1400w,/aicomparts/assets/img/logos/icst-1800.webp 1800w," sizes="95vw" type="image/webp"></source> <img src="/aicomparts/assets/img/logos/icst.png" width="100%" height="auto" title="Institute for Computer Music and Sound Technologies at Zurich University of the Arts, Switzerland" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/aicomparts/assets/bibliography/ramfem.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Kıvanç Tatar. </div> </footer> <script src="/aicomparts/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>