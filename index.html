<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AI in Computational Arts, Music, and Games </title> <meta name="author" content="Kıvanç Tatar"> <meta name="description" content=""> <meta name="keywords" content="Machine Learning, Artificial Intelligence, Computational Arts, Music, Video Games, Computational Creativity, Musical AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.aicomparts.com//"> <script src="/assets/js/theme.js?8c8262f54ac80beb3e456971f921ab9f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> AI in Computational Arts, Music, and Games </h1> <p class="desc">a Research Group @ <a href="https://www.chalmers.se/en/departments/cse/our-research/data-science-and-ai/" rel="external nofollow noopener" target="_blank"> Division of Data Science and AI </a>, <a href="https://www.chalmers.se" rel="external nofollow noopener" target="_blank"> Chalmers University of Technology</a> in <i class="fa-solid fa-cloud-rain"></i> Gothenburg <i class="fa-solid fa-rainbow"></i> , Sweden.</p> </header> <article> <div class="clearfix"> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 27, 2025</th> <td> We have a new PhD position available! -&gt; <a href="https://www.chalmers.se/en/about-chalmers/work-with-us/vacancies/?rmpage=job&amp;rmjob=14069&amp;rmlang=UK" rel="external nofollow noopener" target="_blank">Click here</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 24, 2025</th> <td> <a href="https://aicomparts.com/people/hugh_liu/" rel="external nofollow noopener" target="_blank">Xuechen (Hugh)</a> and <a href="https://aicomparts.com/people/kivanc_tatar/" rel="external nofollow noopener" target="_blank">Kıvanç</a> are visiting Malmö University for the <a href="https://school.gameaibook.org/" rel="external nofollow noopener" target="_blank">Game AI summer school</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 23, 2025</th> <td> <a href="https://aicomparts.com/people/kelsey_cotton/" rel="external nofollow noopener" target="_blank">Kelsey</a> joined a panel on <a href="https://nime2025.org/proceedings/335.html" rel="external nofollow noopener" target="_blank">Somatic and somaesthetic design practices in NIME</a> at NIME 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 03, 2025</th> <td> <a href="https://aicomparts.com/people/kelsey_cotton/" rel="external nofollow noopener" target="_blank">Kelsey</a> is presenting a live coding performance with realtime AI vocals at the <a href="https://www.kth.se/femtech/exhibition-1.1391820" rel="external nofollow noopener" target="_blank">FemTech + Feminist Tech Exhibition</a> at <a href="https://www.kth.se/en/om/upptack/r1/kth-reaktorhallen-1.739170" rel="external nofollow noopener" target="_blank">KTH Reactor Hall</a> in Stockholm, Sweden. <i class="fa-solid fa-microphone-lines"></i> <i class="fa-solid fa-microphone-lines"></i> <i class="fa-solid fa-microphone-lines"></i> </td> </tr> <tr> <th scope="row" style="width: 20%">May 06, 2025</th> <td> The students of our <a href="https://www.chalmers.se/en/education/your-studies/course-selection-and-registration/select-courses/choose-a-tracks-course/#tracks-a-unique-concept" rel="external nofollow noopener" target="_blank">Tracks</a> course <a href="https://aicomparts.com/teaching/" rel="external nofollow noopener" target="_blank">TRA385 Machine Learning and AI through Artistic Innovation</a> will present their artworks at the <a href="https://maps.chalmers.se/#2ee50140-368e-11ee-83e1-850d69ec6121" rel="external nofollow noopener" target="_blank">FUSE Box</a> on May 15, 13:15-16:30. <i class="fa-solid fa-palette"></i><i class="fa-solid fa-music"></i><i class="fa-solid fa-robot"></i><i class="fa-solid fa-camera-retro"></i> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cotton_imploding_2025" class="col-sm-8"> <div class="title">Imploding between the facts and concerns: analysing human–AI musical interaction</div> <div class="author"> Kelsey Cotton , Anna-Kaisa Kaila , Petra Jääskeläinen , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'André Holzapfel, Kıvanç Tatar' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Humanities and Social Sciences Communications</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The advancement of AI-tools for musical performance has inspired exciting opportunities for interaction with musical-AI-agents. Interactions between humans and AI-agents in musical settings entail dynamic exchanges of control and power, and framings of AI-agents’ roles by human performers. We probe these framings and power-control exchanges through qualitative thematic lenses, drawing from post-phenomenology, matters of fact and concern and feminist science and technology studies. We contribute with a novel interdisciplinary analytical method as a tool for developers and designers of AI systems to help visibilise and examine the implicit, the wider connections and entangled filaments in Human–AI musical interactions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cotton_imploding_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Imploding between the facts and concerns: analysing human–AI musical interaction}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2662-9992}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nature.com/articles/s41599-025-04533-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1057/s41599-025-04533-4}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2025-06-03}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Humanities and Social Sciences Communications}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cotton, Kelsey and Kaila, Anna-Kaisa and Jääskeläinen, Petra and Holzapfel, André and Tatar, Kıvanç}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="chen_sounding_2024" class="col-sm-8"> <div class="title">A Deep Learning Framework for Musical Acoustics Simulations</div> <div class="author"> Jiafeng Chen , Kıvanç Tatar , and Victor Zappi </div> <div class="periodical"> <em>In AIMC 2024</em> , Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The acoustic modeling of musical instruments is a heavy computational process, often bound to the solution of complex systems of partial differential equations (PDEs). Numerical models can achieve a high level of accuracy, but they may take up to several hours to complete a full simulation, especially in the case of intricate musical mechanisms. The application of deep learning, and in particular of neural operators that learn mappings between function spaces, has the potential to revolutionize how acoustics PDEs are solved and noticeably speed up musical simulations. However, extensive research is necessary to understand the applicability of such operators in musical acoustics; this requires large datasets, capable of exemplifying the relationship between input parameters (excitation) and output solutions (acoustic wave propagation) per each target musical instrument/configuration. With this work, we present an open-access, open-source framework designed for the generation of numerical musical acoustics datasets and for the training/benchmarking of acoustics neural operators. We first describe the overall structure of the framework and the proposed data generation workflow. Then, we detail the first numerical models that were ported to the framework. This work is a first step towards the gathering of a research community that focuses on deep learning applied to musical acoustics, and shares workflows and benchmarking tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen_sounding_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Deep Learning Framework for Musical Acoustics Simulations}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AIMC 2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Jiafeng and Tatar, Kıvanç and Zappi, Victor}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aimc2024.pubpub.org/pub/5cl1cvmy/release/1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cotton_sounding_2024" class="col-sm-8"> <div class="title">Sounding out extra-normal AI voice: Non-normative musical engagements with normative AI voice and speech technologies</div> <div class="author"> Kelsey Cotton , and Kıvanç Tatar </div> <div class="periodical"> <em>In AIMC 2024</em> , Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>How do we challenge the norms of AI voice technologies? What would be a non-normative approach in finding novel artistic possibilities of speech synthesis and text-to-speech with Deep Learning? This paper delves into SpeechBrain, OpenAI and CoquiTTS voice and speech models with the perspective of an experimental vocal practitioner. Exploratory Research-through-Design guided an engagement with pre-trained speech synthesis models to reveal their musical affordances in an experimental vocal practice. We recorded this engagement with voice and speech Deep Learning technologies using auto-ethnography, a novel and recent methodology in Human-Computer Interaction. Our position in this paper actively subverts the normative function of these models, provoking nonsensical AI-mediation of human vocality. Emerging from a sense-making process of poetic AI nonsense, we uncover the generative potential of non-normative usage of normative speech recognition and synthesis models. We contribute with insights about the affordances of Research-through-Design to inform artistic working processes with AI models; how AI-mediations reform understandings of human vocality; and artistic perspectives and practice as knowledge-creation mechanisms for working with technology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cotton_sounding_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sounding out extra-normal AI voice: Non-normative musical engagements with normative AI voice and speech technologies}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AIMC 2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cotton, Kelsey and Tatar, Kıvanç}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aimc2024.pubpub.org/pub/extranormal-aivoice/release/1}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cotton_singing_2024" class="col-sm-8"> <div class="title">Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies</div> <div class="author"> Kelsey Cotton , Katja Vries , and Kıvanç Tatar </div> <div class="periodical"> <em>In Proceedings of the 9th International Conference on Movement and Computing</em> , Utrecht, Netherlands, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Technological advancements in deep learning for speech and voice have contributed to a recent expansion in applications for voice cloning, synthesis and generation. Invisibilised stakeholders in this expansion are numerous absent bodies, whose voices and voice data have been integral to the development and refinement of these speech technologies. This position paper probes current working practices for voice and speech in machine learning and AI, in which the bodies of voices are “invisibilised". We examine the facts and concerns about the voice-Body in applications of AI-voice technology. We do this through probing the wider connections between voice data and Schaefferian listening; speculating on the consequences of missing Bodies in AI-Voice; and by examining how vocalists and artists working with synthetic Bodies and AI-voices are ‘bringing the Body back’ in their own practices. We contribute with a series of considerations for how practitioners and researchers may help to ‘bring the Body back’ into AI-voice technologies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cotton_singing_2024</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cotton, Kelsey and de Vries, Katja and Tatar, Kıvanç}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400709944}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3658852.3659065}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3658852.3659065}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 9th International Conference on Movement and Computing}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{AI, STS, artificial intelligence, body, musical AI, voice}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Utrecht, Netherlands}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MOCO '24}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="tatar_shift_2024" class="col-sm-8"> <div class="title">A Shift in Artistic Practices through Artificial Intelligence</div> <div class="author"> Kıvanç Tatar , Petter Ericson , Kelsey Cotton , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Paola Torres Núñez Del Prado, Roser Batlle-Roca, Beatriz Cabrero-Daniel, Sara Ljungblad, Georgios Diapoulis, Jabbar Hussain' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Leonardo</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/pdf/2306.10054" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The explosion of content generated by artificial intelligence (AI) models has initiated a cultural shift in arts, music, and media, whereby roles are changing, values are shifting, and conventions are challenged. The vast, readily available dataset of the Internet has created an environment for AI models to be trained on any content on the Web. With AI models shared openly and used by many globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring to music, arts, and new media?</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tatar_shift_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Shift in Artistic Practices through Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0024-094X}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1162/leon_a_02523}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/leon_a_02523}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-09}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Leonardo}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tatar, Kıvanç and Ericson, Petter and Cotton, Kelsey and Del Prado, Paola Torres Núñez and Batlle-Roca, Roser and Cabrero-Daniel, Beatriz and Ljungblad, Sara and Diapoulis, Georgios and Hussain, Jabbar}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{293--297}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cotton_imploding_2026" class="col-sm-8"> <div class="title">Imploding between the facts and concerns: analysing human–AI musical interaction</div> <div class="author"> Kelsey Cotton , Anna-Kaisa Kaila , Petra Jääskeläinen , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'André Holzapfel, Kıvanç Tatar' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Humanities and Social Sciences Communications</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The advancement of AI-tools for musical performance has inspired exciting opportunities for interaction with musical-AI-agents. Interactions between humans and AI-agents in musical settings entail dynamic exchanges of control and power, and framings of AI-agents’ roles by human performers. We probe these framings and power-control exchanges through qualitative thematic lenses, drawing from post-phenomenology, matters of fact and concern and feminist science and technology studies. We contribute with a novel interdisciplinary analytical method as a tool for developers and designers of AI systems to help visibilise and examine the implicit, the wider connections and entangled filaments in Human–AI musical interactions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cotton_imploding_2026</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Imploding between the facts and concerns: analysing human–AI musical interaction}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2662-9992}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nature.com/articles/s41599-025-04533-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1057/s41599-025-04533-4}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2025-06-03}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Humanities and Social Sciences Communications}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cotton, Kelsey and Kaila, Anna-Kaisa and Jääskeläinen, Petra and Holzapfel, André and Tatar, Kıvanç}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="chen_sounding_2025" class="col-sm-8"> <div class="title">A Deep Learning Framework for Musical Acoustics Simulations</div> <div class="author"> Jiafeng Chen , Kıvanç Tatar , and Victor Zappi </div> <div class="periodical"> <em>In AIMC 2024</em> , Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The acoustic modeling of musical instruments is a heavy computational process, often bound to the solution of complex systems of partial differential equations (PDEs). Numerical models can achieve a high level of accuracy, but they may take up to several hours to complete a full simulation, especially in the case of intricate musical mechanisms. The application of deep learning, and in particular of neural operators that learn mappings between function spaces, has the potential to revolutionize how acoustics PDEs are solved and noticeably speed up musical simulations. However, extensive research is necessary to understand the applicability of such operators in musical acoustics; this requires large datasets, capable of exemplifying the relationship between input parameters (excitation) and output solutions (acoustic wave propagation) per each target musical instrument/configuration. With this work, we present an open-access, open-source framework designed for the generation of numerical musical acoustics datasets and for the training/benchmarking of acoustics neural operators. We first describe the overall structure of the framework and the proposed data generation workflow. Then, we detail the first numerical models that were ported to the framework. This work is a first step towards the gathering of a research community that focuses on deep learning applied to musical acoustics, and shares workflows and benchmarking tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen_sounding_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Deep Learning Framework for Musical Acoustics Simulations}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AIMC 2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Jiafeng and Tatar, Kıvanç and Zappi, Victor}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aimc2024.pubpub.org/pub/5cl1cvmy/release/1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cotton_sounding_2025" class="col-sm-8"> <div class="title">Sounding out extra-normal AI voice: Non-normative musical engagements with normative AI voice and speech technologies</div> <div class="author"> Kelsey Cotton , and Kıvanç Tatar </div> <div class="periodical"> <em>In AIMC 2024</em> , Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>How do we challenge the norms of AI voice technologies? What would be a non-normative approach in finding novel artistic possibilities of speech synthesis and text-to-speech with Deep Learning? This paper delves into SpeechBrain, OpenAI and CoquiTTS voice and speech models with the perspective of an experimental vocal practitioner. Exploratory Research-through-Design guided an engagement with pre-trained speech synthesis models to reveal their musical affordances in an experimental vocal practice. We recorded this engagement with voice and speech Deep Learning technologies using auto-ethnography, a novel and recent methodology in Human-Computer Interaction. Our position in this paper actively subverts the normative function of these models, provoking nonsensical AI-mediation of human vocality. Emerging from a sense-making process of poetic AI nonsense, we uncover the generative potential of non-normative usage of normative speech recognition and synthesis models. We contribute with insights about the affordances of Research-through-Design to inform artistic working processes with AI models; how AI-mediations reform understandings of human vocality; and artistic perspectives and practice as knowledge-creation mechanisms for working with technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cotton_singing_2025" class="col-sm-8"> <div class="title">Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies</div> <div class="author"> Kelsey Cotton , Katja Vries , and Kıvanç Tatar </div> <div class="periodical"> <em>In Proceedings of the 9th International Conference on Movement and Computing</em> , Utrecht, Netherlands, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Technological advancements in deep learning for speech and voice have contributed to a recent expansion in applications for voice cloning, synthesis and generation. Invisibilised stakeholders in this expansion are numerous absent bodies, whose voices and voice data have been integral to the development and refinement of these speech technologies. This position paper probes current working practices for voice and speech in machine learning and AI, in which the bodies of voices are “invisibilised". We examine the facts and concerns about the voice-Body in applications of AI-voice technology. We do this through probing the wider connections between voice data and Schaefferian listening; speculating on the consequences of missing Bodies in AI-Voice; and by examining how vocalists and artists working with synthetic Bodies and AI-voices are ‘bringing the Body back’ in their own practices. We contribute with a series of considerations for how practitioners and researchers may help to ‘bring the Body back’ into AI-voice technologies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cotton_singing_2025</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cotton, Kelsey and de Vries, Katja and Tatar, Kıvanç}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400709944}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery (ACM)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3658852.3659065}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3658852.3659065}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 9th International Conference on Movement and Computing}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{AI, STS, artificial intelligence, body, musical AI, voice}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Utrecht, Netherlands}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MOCO '24}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="tatar_shift_2025" class="col-sm-8"> <div class="title">A Shift in Artistic Practices through Artificial Intelligence</div> <div class="author"> Kıvanç Tatar , Petter Ericson , Kelsey Cotton , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Paola Torres Núñez Del Prado, Roser Batlle-Roca, Beatriz Cabrero-Daniel, Sara Ljungblad, Georgios Diapoulis, Jabbar Hussain' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Leonardo</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/pdf/2306.10054" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The explosion of content generated by artificial intelligence (AI) models has initiated a cultural shift in arts, music, and media, whereby roles are changing, values are shifting, and conventions are challenged. The vast, readily available dataset of the Internet has created an environment for AI models to be trained on any content on the Web. With AI models shared openly and used by many globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring to music, arts, and new media?</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tatar_shift_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Shift in Artistic Practices through Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0024-094X}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1162/leon_a_02523}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/leon_a_02523}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-09}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Leonardo}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MIT Press}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tatar, Kıvanç and Ericson, Petter and Cotton, Kelsey and Del Prado, Paola Torres Núñez and Batlle-Roca, Roser and Cabrero-Daniel, Beatriz and Ljungblad, Sara and Diapoulis, Georgios and Hussain, Jabbar}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{293--297}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%74%61%74%61%72@%63%68%61%6C%6D%65%72%73.%73%65" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Kıvanç Tatar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?9b6fc7aeb2b96b27ea68c9fa31a41c0c"></script> <script defer src="/assets/js/copy_code.js?d359581efc54b08366f9ef8219e6e511" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>