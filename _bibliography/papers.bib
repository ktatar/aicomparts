---
---
@inproceedings{cotton_caring_2023,
	title = {Caring {Trouble} and {Musical} {AI}: {Considerations} towards a {Feminist} {Musical} {AI}},
	shorttitle = {Caring {Trouble} and {Musical} {AI}},
	url = {https://aimc2023.pubpub.org/pub/zwjy371l/release/1},
	abstract = {The ethics of AI as both material and medium for interaction remains in murky waters within the context of musical and artistic practice. The interdisciplinarity of the field is revealing matters of concern and care, which necessitate interdisciplinary methodologies for evaluation to trouble and critique the inheritance of ‘residue-laden’ AI-tools in musical applications. Seeking to unsettle these murky waters, this paper critically examines the example of Holly+, a deep neural network that generates raw audio in the likeness of its creator Holly Herndon. Drawing from theoretical concerns and considerations from speculative feminism and care ethics, we care-fully trouble the structures, frameworks and assumptions that oscillate within and around Holly+. We contribute with several considerations and contemplate future directions for integrating speculative feminism and care into musical-AI agent and system design, derived from our critical feminist examination.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {{AIMC} 2023},
	author = {Cotton, Kelsey and Tatar, Kıvanç},
	month = aug,
	year = {2023},
}

@inproceedings{tatar_sound_2023,
	title = {Sound {Design} {Strategies} for {Latent} {Audio} {Space} {Explorations} {Using} {Deep} {Learning} {Architectures}},
	abstract = {The research in Deep Learning applications in sound and music computing have gathered an interest in the recent years; however, there is still a missing link between these new technologies and on how they can be incorporated into real-world artistic practices. In this work, we explore a well-known Deep Learning architecture called Variational Autoencoders (VAEs). These architectures have been used in many areas for generating latent spaces where data points are organized so that similar data points locate closer to each other. Previously, VAEs have been used for generating latent timbre spaces or latent spaces of symbolic music excepts. Applying VAE to audio features of timbre requires a vocoder to transform the timbre generated by the network to an audio signal, which is computationally expensive. In this work, we apply VAEs to raw audio data directly while bypassing audio feature extraction. This approach allows the practitioners to use any audio recording while giving flexibility and control over the aesthetics through dataset curation. The lower computation time in audio signal generation allows the raw audio approach to be incorporated into real-time applications. In this work, we propose three strategies to explore latent spaces of audio and timbre for sound design applications. By doing so, our aim is to initiate a conversation on artistic approaches and strategies to utilize latent audio spaces in sound and music practices.},
	language = {en},
	author = {Tatar, Kıvanc and Cotton, Kelsey and Bisig, Daniel},
	year = {2023},
}

@phdthesis{tatar_musical_2019,
	type = {Thesis},
	title = {Musical agents based on self-organizing maps for audio applications},
	url = {http://summit.sfu.ca/item/19665},
	language = {en},
	urldate = {2020-06-03},
	school = {Communication, Art \& Technology: School of Interactive Arts and Technology},
	author = {Tatar, Kivanc},
	month = may,
	year = {2019},
}

@inproceedings{bisig_raw_2021,
	title = {Raw {Music} from {Free} {Movements}: {Early} {Experiments} in {Using} {Machine} {Learning} to {Create} {Raw} {Audio} from {Dance} {Movements}},
	abstract = {Raw Music from Free Movements is a deep learning architecture that translates pose sequences into audio waveforms. The architecture combines a sequence-to-sequence model generating audio encodings and an adversarial autoencoder that generates raw audio from audio encodings. Experiments have been conducted with two datasets: a dancer improvising freely to a given music, and music created through simple movement soniﬁcation. The paper presents preliminary results. These will hopefully lead closer towards a model which can learn from the creative decisions a dancer makes when translating music into movement and then follow these decisions reversely for the purpose of generating music from movement.},
	language = {en},
	booktitle = {Proceedings of the 2nd {AI} {Music} {Creativity} {Conference} ({AIMC} 2021)},
	author = {Bisig, Daniel and Tatar, Kıvanc},
	year = {2021},
	pages = {11},
}

@misc{dignum_importance_2023,
	title = {On the importance of {AI} research beyond disciplines},
	url = {http://arxiv.org/abs/2302.06655},
	doi = {10.48550/arXiv.2302.06655},
	abstract = {As the impact of AI on various scientific fields is increasing, it is crucial to embrace interdisciplinary knowledge to understand the impact of technology on society. The goal is to foster a research environment beyond disciplines that values diversity and creates, critiques and develops new conceptual and theoretical frameworks. Even though research beyond disciplines is essential for understanding complex societal issues and creating positive impact it is notoriously difficult to evaluate and is often not recognized by current academic career progression. The motivation for this paper is to engage in broad discussion across disciplines and identify guiding principles fir AI research beyond disciplines in a structured and inclusive way, revealing new perspectives and contributing to societal and human wellbeing and sustainability.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Dignum, Virginia and Casey, Donal and Cerratto-Pargman, Teresa and Dignum, Frank and Fantasia, Valentina and Formark, Bodil and Hammarfelt, Björn and Holmberg, Gunnar and Holzapfel, André and Larsson, Stefan and Lagerkvist, Amanda and Lakemond, Nicolette and Lindgren, Helena and Lorig, Fabian and Marusic, Ana and Rahm, Lina and Razmetaeva, Yulia and Sikström, Sverker and Tatar, Kıvanç and Tucker, Jason},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06655 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@article{tatar_latent_2020,
	title = {Latent {Timbre} {Synthesis}},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-020-05424-2},
	doi = {10.1007/s00521-020-05424-2},
	abstract = {We present the Latent Timbre Synthesis, a new audio synthesis method using deep learning. The synthesis method allows composers and sound designers to interpolate and extrapolate between the timbre of multiple sounds using the latent space of audio frames. We provide the details of two Variational Autoencoder architectures for the Latent Timbre Synthesis and compare their advantages and drawbacks. The implementation includes a fully working application with a graphical user interface, called interpolate\_two, which enables practitioners to generate timbres between two audio excerpts of their selection using interpolation and extrapolation in the latent space of audio frames. Our implementation is open source, and we aim to improve the accessibility of this technology by providing a guide for users with any technical background. Our study includes a qualitative analysis where nine composers evaluated the Latent Timbre Synthesis and the interpolate\_two application within their practices.},
	language = {en},
	urldate = {2020-11-09},
	journal = {Neural Computing and Applications},
	author = {Tatar, Kıvanç and Bisig, Daniel and Pasquier, Philippe},
	month = oct,
	year = {2020},
}

@article{tatar_shift_2024,
	title = {A {Shift} in {Artistic} {Practices} through {Artificial} {Intelligence}},
	issn = {0024-094X},
	url = {https://doi.org/10.1162/leon_a_02523},
	doi = {10.1162/leon_a_02523},
	abstract = {The explosion of content generated by artificial intelligence (AI) models has initiated a cultural shift in arts, music, and media, whereby roles are changing, values are shifting, and conventions are challenged. The vast, readily available dataset of the Internet has created an environment for AI models to be trained on any content on the Web. With AI models shared openly and used by many globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring to music, arts, and new media?},
	urldate = {2024-04-09},
	journal = {Leonardo},
	author = {Tatar, Kivanç and Ericson, Petter and Cotton, Kelsey and Del Prado, Paola Torres Núñez and Batlle-Roca, Roser and Cabrero-Daniel, Beatriz and Ljungblad, Sara and Diapoulis, Georgios and Hussain, Jabbar},
	month = apr,
	year = {2024},
	pages = {293--297},
  selected={true},
}
